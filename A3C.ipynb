{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# A3C\n",
    "\n",
    "A3C is an parallel version of A2C. It introduces asynchronous updates of the policy from $N$ agents running on the same multicore machine.\n",
    "Here is the brief explanation of the logic:\n",
    "\n",
    "First the global policy-value network parameters $\\theta$ are randomly initialized. Network parameters are made available to all the processes.\n",
    "\n",
    "Every agent runs in a separater process/thread. It acts as an advantage actor critic (A2C). It repeats the following set of steps till convergence:\n",
    "1. Pull the latest parameters from the global network. $\\theta_{\\mathcal{A}_i}\\leftarrow\\theta$\n",
    "2. Run policy in the local environment for a number of steps T\n",
    "3. Calculate the loss $\\mathcal{L_\\theta_{\\mathcal{A}_i}}=\\mathcal{L_\\theta_{\\mathcal{A}_i}}^{POLICY}+\\mathcal{L_\\theta_{\\mathcal{A}_i}}^{VALUE}$ using A2C logic.\n",
    "4. Calculate the gradient $\\nabla_\\theta_{\\mathcal{A}_i} \\mathcal{L_\\theta_{\\mathcal{A}_i}}$\n",
    "5. Update the global network with the gradient. $\\theta = \\theta - \\alpha*\\nabla_\\theta_{\\mathcal{A}_i} \\mathcal{L_\\theta_{\\mathcal{A}_i}}$\n",
    "\n",
    "At the first glance A3C should converge $N$ times faster than A2C, but it turns out that in practice it converges even faster. It's not clear why, maybe it's because of decorrelated updates of network parameters?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from gym.spaces import Box\n",
    "\n",
    "import A3C"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n",
      "[INFO/MainProcess] Creating policy. is_continuous=False\n"
     ]
    },
    {
     "data": {
      "text/plain": "(Categorical(probs: torch.Size([2]), logits: torch.Size([2])),\n array(0),\n tensor(-0.7704, grad_fn=<SqueezeBackward1>),\n tensor([-0.4709], grad_fn=<AddBackward0>))"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = A3C.PolicyValueModel(2, 2, 4)\n",
    "model.forward([1, 2.])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "SharedAdam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    lr: 0.001\n    maximize: False\n    weight_decay: 0\n)"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim = A3C.SharedAdam(model.parameters())\n",
    "optim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "importlib.reload(A3C)\n",
    "\n",
    "\n",
    "def a3c(env_name, capacity=256, n_agents=1, early_stopping_reward=300, num_episodes=10000, train_every_step=16,\n",
    "        max_episode_length=300, entropy_loss_weight=0.01, gamma=0.9):\n",
    "\n",
    "    with gym.make(env_name) as env:\n",
    "        n = env.action_space.shape[0] if type(env.action_space) is Box else env.action_space.n\n",
    "        global_model = A3C.PolicyValueModel(env.observation_space.shape[0], n, capacity,\n",
    "                                            type(env.action_space) is Box)\n",
    "        optimizer = A3C.SharedAdam(global_model.parameters())\n",
    "        agents = []\n",
    "        for a in range(n_agents):\n",
    "            agents.append(A3C.A3CAgent(f'agent{a}', env_name, capacity, global_model, optimizer, num_episodes,\n",
    "                                       train_every_step, early_stopping_reward, gamma, entropy_loss_weight,\n",
    "                                       max_episode_length, verbose=a == 0, terminate_flag=mp.Value(ctypes.c_int, 0)))\n",
    "            agents[-1].start()\n",
    "        for a in agents:\n",
    "            a.join()\n",
    "        return global_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CartPole-v1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/agent0] child process calling self.run()\n",
      "[INFO/agent1] child process calling self.run()\n",
      "[INFO/agent0] child process calling self.run()\n",
      "[INFO/agent0] Agent agent0 starting\n",
      "[INFO/agent0] Agent agent0 starting\n",
      "[INFO/agent1] child process calling self.run()\n",
      "[INFO/agent1] Agent agent1 starting\n",
      "[INFO/agent1] Agent agent1 starting\n",
      "[INFO/agent3] child process calling self.run()\n",
      "[INFO/agent3] child process calling self.run()\n",
      "[INFO/agent3] Agent agent3 starting\n",
      "[INFO/agent3] Agent agent3 starting\n",
      "[INFO/agent4] child process calling self.run()\n",
      "[INFO/agent4] child process calling self.run()\n",
      "[INFO/agent4] Agent agent4 starting\n",
      "[INFO/agent4] Agent agent4 starting\n",
      "[INFO/agent5] child process calling self.run()\n",
      "[INFO/agent5] child process calling self.run()\n",
      "[INFO/agent5] Agent agent5 starting\n",
      "[INFO/agent5] Agent agent5 starting\n",
      "[INFO/agent6] child process calling self.run()\n",
      "[INFO/agent6] child process calling self.run()\n",
      "[INFO/agent6] Agent agent6 starting\n",
      "[INFO/agent6] Agent agent6 starting\n",
      "[INFO/agent7] child process calling self.run()\n",
      "[INFO/agent7] child process calling self.run()\n",
      "[INFO/agent7] Agent agent7 starting\n",
      "[INFO/agent7] Agent agent7 starting\n",
      "[INFO/agent8] child process calling self.run()\n",
      "[INFO/agent8] child process calling self.run()\n",
      "[INFO/agent8] Agent agent8 starting\n",
      "[INFO/agent8] Agent agent8 starting\n",
      "[INFO/agent3] agent3 episode:1; loss:00.385; episode len:000015; total reward:000016 entropy:00.685 \n",
      "[INFO/agent3] agent3 episode:1; loss:00.385; episode len:000015; total reward:000016 entropy:00.685 \n",
      "[INFO/agent2] child process calling self.run()\n",
      "[INFO/agent2] child process calling self.run()\n",
      "[INFO/agent2] Agent agent2 starting\n",
      "[INFO/agent2] Agent agent2 starting\n",
      "[INFO/agent9] child process calling self.run()\n",
      "[INFO/agent9] child process calling self.run()\n",
      "[INFO/agent9] Agent agent9 starting\n",
      "[INFO/agent9] Agent agent9 starting\n",
      "[INFO/agent10] child process calling self.run()\n",
      "[INFO/agent10] child process calling self.run()\n",
      "[INFO/agent10] Agent agent10 starting\n",
      "[INFO/agent10] Agent agent10 starting\n",
      "[INFO/agent0] agent0 episode:1; loss:00.129; episode len:000040; total reward:000040 entropy:00.682 \n",
      "[INFO/agent0] agent0 episode:1; loss:00.129; episode len:000040; total reward:000040 entropy:00.682 \n",
      "[INFO/agent6] agent6 episode:1; loss:00.333; episode len:000014; total reward:000016 entropy:00.686 \n",
      "[INFO/agent6] agent6 episode:1; loss:00.333; episode len:000014; total reward:000016 entropy:00.686 \n",
      "[INFO/agent11] child process calling self.run()\n",
      "[INFO/agent11] child process calling self.run()\n",
      "[INFO/agent11] Agent agent11 starting\n",
      "[INFO/agent11] Agent agent11 starting\n",
      "[INFO/agent6] agent6 episode:3; loss:-0.313; episode len:000018; total reward:000020 entropy:00.689 \n",
      "[INFO/agent6] agent6 episode:3; loss:-0.313; episode len:000018; total reward:000020 entropy:00.689 \n",
      "[INFO/agent2] agent2 episode:5; loss:-0.312; episode len:000016; total reward:000017 entropy:00.685 \n",
      "[INFO/agent2] agent2 episode:5; loss:-0.312; episode len:000016; total reward:000017 entropy:00.685 \n",
      "[INFO/agent7] agent7 episode:4; loss:-0.485; episode len:000022; total reward:000023 entropy:00.684 \n",
      "[INFO/agent7] agent7 episode:4; loss:-0.485; episode len:000022; total reward:000023 entropy:00.684 \n",
      "[INFO/agent7] agent7 episode:5; loss:-0.325; episode len:000021; total reward:000022 entropy:00.678 \n",
      "[INFO/agent7] agent7 episode:5; loss:-0.325; episode len:000021; total reward:000022 entropy:00.678 \n",
      "[INFO/agent10] agent10 episode:8; loss:-0.412; episode len:000023; total reward:000024 entropy:00.664 \n",
      "[INFO/agent10] agent10 episode:8; loss:-0.412; episode len:000023; total reward:000024 entropy:00.664 \n",
      "[INFO/agent4] agent4 episode:13; loss:00.418; episode len:000018; total reward:000019 entropy:00.638 \n",
      "[INFO/agent4] agent4 episode:13; loss:00.418; episode len:000018; total reward:000019 entropy:00.638 \n",
      "[INFO/agent11] agent11 episode:9; loss:-0.037; episode len:000031; total reward:000032 entropy:00.646 \n",
      "[INFO/agent11] agent11 episode:9; loss:-0.037; episode len:000031; total reward:000032 entropy:00.646 \n",
      "[INFO/agent0] agent0 episode:15; loss:00.041; episode len:000027; total reward:000028 entropy:00.646 \n",
      "[INFO/agent0] agent0 episode:15; loss:00.041; episode len:000027; total reward:000028 entropy:00.646 \n",
      "[INFO/agent7] agent7 episode:15; loss:-0.902; episode len:000027; total reward:000028 entropy:00.647 \n",
      "[INFO/agent7] agent7 episode:15; loss:-0.902; episode len:000027; total reward:000028 entropy:00.647 \n",
      "[INFO/agent3] agent3 episode:19; loss:00.584; episode len:000025; total reward:000026 entropy:00.635 \n",
      "[INFO/agent3] agent3 episode:19; loss:00.584; episode len:000025; total reward:000026 entropy:00.635 \n",
      "[INFO/agent2] agent2 episode:16; loss:00.902; episode len:000037; total reward:000038 entropy:00.617 \n",
      "[INFO/agent2] agent2 episode:16; loss:00.902; episode len:000037; total reward:000038 entropy:00.617 \n",
      "[INFO/agent5] agent5 episode:21; loss:01.865; episode len:000041; total reward:000042 entropy:00.639 \n",
      "[INFO/agent5] agent5 episode:21; loss:01.865; episode len:000041; total reward:000042 entropy:00.639 \n",
      "[INFO/agent10] agent10 episode:19; loss:-0.555; episode len:000048; total reward:000049 entropy:00.652 \n",
      "[INFO/agent10] agent10 episode:19; loss:-0.555; episode len:000048; total reward:000049 entropy:00.652 \n",
      "[INFO/agent2] agent2 episode:23; loss:-0.355; episode len:000045; total reward:000046 entropy:00.599 \n",
      "[INFO/agent2] agent2 episode:23; loss:-0.355; episode len:000045; total reward:000046 entropy:00.599 \n",
      "[INFO/agent3] agent3 episode:29; loss:00.255; episode len:000048; total reward:000049 entropy:00.633 \n",
      "[INFO/agent3] agent3 episode:29; loss:00.255; episode len:000048; total reward:000049 entropy:00.633 \n",
      "[INFO/agent3] agent3 episode:33; loss:03.318; episode len:000058; total reward:000059 entropy:00.540 \n",
      "[INFO/agent3] agent3 episode:33; loss:03.318; episode len:000058; total reward:000059 entropy:00.540 \n",
      "[INFO/agent2] agent2 episode:27; loss:01.305; episode len:000058; total reward:000059 entropy:00.627 \n",
      "[INFO/agent2] agent2 episode:27; loss:01.305; episode len:000058; total reward:000059 entropy:00.627 \n",
      "[INFO/agent9] agent9 episode:29; loss:01.456; episode len:000090; total reward:000091 entropy:00.605 \n",
      "[INFO/agent9] agent9 episode:29; loss:01.456; episode len:000090; total reward:000091 entropy:00.605 \n",
      "[INFO/agent6] agent6 episode:33; loss:02.717; episode len:000101; total reward:000102 entropy:00.610 \n",
      "[INFO/agent6] agent6 episode:33; loss:02.717; episode len:000101; total reward:000102 entropy:00.610 \n",
      "[INFO/agent7] agent7 episode:31; loss:01.978; episode len:000092; total reward:000094 entropy:00.611 \n",
      "[INFO/agent7] agent7 episode:31; loss:01.978; episode len:000092; total reward:000094 entropy:00.611 \n",
      "[INFO/agent2] agent2 episode:35; loss:03.190; episode len:000097; total reward:000098 entropy:00.574 \n",
      "[INFO/agent2] agent2 episode:35; loss:03.190; episode len:000097; total reward:000098 entropy:00.574 \n",
      "[INFO/agent8] agent8 episode:34; loss:04.299; episode len:000098; total reward:000099 entropy:00.577 \n",
      "[INFO/agent8] agent8 episode:34; loss:04.299; episode len:000098; total reward:000099 entropy:00.577 \n",
      "[INFO/agent11] agent11 episode:33; loss:01.668; episode len:000115; total reward:000116 entropy:00.589 \n",
      "[INFO/agent11] agent11 episode:33; loss:01.668; episode len:000115; total reward:000116 entropy:00.589 \n",
      "[INFO/agent8] agent8 episode:36; loss:04.225; episode len:000096; total reward:000111 entropy:00.553 \n",
      "[INFO/agent8] agent8 episode:36; loss:04.225; episode len:000096; total reward:000111 entropy:00.553 \n",
      "[INFO/agent7] agent7 episode:36; loss:-0.060; episode len:000122; total reward:000122 entropy:00.632 \n",
      "[INFO/agent7] agent7 episode:36; loss:-0.060; episode len:000122; total reward:000122 entropy:00.632 \n",
      "[INFO/agent9] agent9 episode:36; loss:10.001; episode len:000151; total reward:000152 entropy:00.514 \n",
      "[INFO/agent9] agent9 episode:36; loss:10.001; episode len:000151; total reward:000152 entropy:00.514 \n",
      "[INFO/agent4] agent4 episode:44; loss:-0.296; episode len:000149; total reward:000150 entropy:00.580 \n",
      "[INFO/agent4] agent4 episode:44; loss:-0.296; episode len:000149; total reward:000150 entropy:00.580 \n",
      "[INFO/agent0] agent0 episode:34; loss:-0.327; episode len:000166; total reward:000166 entropy:00.614 \n",
      "[INFO/agent0] agent0 episode:34; loss:-0.327; episode len:000166; total reward:000166 entropy:00.614 \n",
      "[INFO/agent3] agent3 episode:44; loss:00.437; episode len:000145; total reward:000162 entropy:00.527 \n",
      "[INFO/agent3] agent3 episode:44; loss:00.437; episode len:000145; total reward:000162 entropy:00.527 \n",
      "[INFO/agent7] agent7 episode:42; loss:00.056; episode len:000155; total reward:000156 entropy:00.542 \n",
      "[INFO/agent7] agent7 episode:42; loss:00.056; episode len:000155; total reward:000156 entropy:00.542 \n",
      "[INFO/agent8] agent8 Total rewards > 200, early stopping!\n",
      "[INFO/agent8] agent8 Total rewards > 200, early stopping!\n",
      "[INFO/agent8] process shutting down\n",
      "[INFO/agent8] process shutting down\n",
      "[INFO/agent8] process exiting with exitcode 0\n",
      "[INFO/agent8] process exiting with exitcode 0\n",
      "[INFO/agent10] agent10 Total rewards > 200, early stopping!\n",
      "[INFO/agent10] agent10 Total rewards > 200, early stopping!\n",
      "[INFO/agent10] process shutting down\n",
      "[INFO/agent10] process shutting down\n",
      "[INFO/agent10] process exiting with exitcode 0\n",
      "[INFO/agent10] process exiting with exitcode 0\n",
      "[INFO/agent4] agent4 Total rewards > 200, early stopping!\n",
      "[INFO/agent4] agent4 Total rewards > 200, early stopping!\n",
      "[INFO/agent4] process shutting down\n",
      "[INFO/agent4] process shutting down\n",
      "[INFO/agent4] process exiting with exitcode 0\n",
      "[INFO/agent4] process exiting with exitcode 0\n",
      "[INFO/agent1] agent1 episode:50; loss:00.007; episode len:000173; total reward:000181 entropy:00.558 \n",
      "[INFO/agent1] agent1 episode:50; loss:00.007; episode len:000173; total reward:000181 entropy:00.558 \n",
      "[INFO/agent11] agent11 Total rewards > 200, early stopping!\n",
      "[INFO/agent11] agent11 Total rewards > 200, early stopping!\n",
      "[INFO/agent11] process shutting down\n",
      "[INFO/agent11] process shutting down\n",
      "[INFO/agent11] process exiting with exitcode 0\n",
      "[INFO/agent11] process exiting with exitcode 0\n",
      "[INFO/agent3] agent3 Total rewards > 200, early stopping!\n",
      "[INFO/agent3] agent3 Total rewards > 200, early stopping!\n",
      "[INFO/agent3] process shutting down\n",
      "[INFO/agent3] process shutting down\n",
      "[INFO/agent3] process exiting with exitcode 0\n",
      "[INFO/agent3] process exiting with exitcode 0\n",
      "[INFO/agent9] agent9 Total rewards > 200, early stopping!\n",
      "[INFO/agent9] agent9 Total rewards > 200, early stopping!\n",
      "[INFO/agent9] process shutting down\n",
      "[INFO/agent9] process shutting down\n",
      "[INFO/agent9] process exiting with exitcode 0\n",
      "[INFO/agent9] process exiting with exitcode 0\n",
      "[INFO/agent1] agent1 Total rewards > 200, early stopping!\n",
      "[INFO/agent1] agent1 Total rewards > 200, early stopping!\n",
      "[INFO/agent1] process shutting down\n",
      "[INFO/agent1] process shutting down\n",
      "[INFO/agent1] process exiting with exitcode 0\n",
      "[INFO/agent1] process exiting with exitcode 0\n",
      "[INFO/agent0] agent0 Total rewards > 200, early stopping!\n",
      "[INFO/agent0] agent0 Total rewards > 200, early stopping!\n",
      "[INFO/agent0] process shutting down\n",
      "[INFO/agent0] process shutting down\n",
      "[INFO/agent0] process exiting with exitcode 0\n",
      "[INFO/agent0] process exiting with exitcode 0\n",
      "[INFO/agent5] agent5 Total rewards > 200, early stopping!\n",
      "[INFO/agent5] agent5 Total rewards > 200, early stopping!\n",
      "[INFO/agent5] process shutting down\n",
      "[INFO/agent5] process shutting down\n",
      "[INFO/agent5] process exiting with exitcode 0\n",
      "[INFO/agent5] process exiting with exitcode 0\n",
      "[INFO/agent2] agent2 Total rewards > 200, early stopping!\n",
      "[INFO/agent2] agent2 Total rewards > 200, early stopping!\n",
      "[INFO/agent2] process shutting down\n",
      "[INFO/agent2] process shutting down\n",
      "[INFO/agent2] process exiting with exitcode 0\n",
      "[INFO/agent2] process exiting with exitcode 0\n",
      "[INFO/agent7] agent7 Total rewards > 200, early stopping!\n",
      "[INFO/agent7] agent7 Total rewards > 200, early stopping!\n",
      "[INFO/agent7] process shutting down\n",
      "[INFO/agent7] process shutting down\n",
      "[INFO/agent7] process exiting with exitcode 0\n",
      "[INFO/agent7] process exiting with exitcode 0\n",
      "[INFO/agent6] agent6 Total rewards > 200, early stopping!\n",
      "[INFO/agent6] agent6 Total rewards > 200, early stopping!\n",
      "[INFO/agent6] process shutting down\n",
      "[INFO/agent6] process shutting down\n",
      "[INFO/agent6] process exiting with exitcode 0\n",
      "[INFO/agent6] process exiting with exitcode 0\n"
     ]
    }
   ],
   "source": [
    "model = a3c('CartPole-v1',\n",
    "            128,\n",
    "            n_agents=os.cpu_count(),\n",
    "            train_every_step=5,\n",
    "            gamma=0.9,\n",
    "            entropy_loss_weight=0.,\n",
    "            early_stopping_reward=200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "with gym.make('CartPole-v1') as env:\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = model.forward(torch.tensor(state))[1]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LunarLanderContinuous-v2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/agent1] child process calling self.run()\n",
      "[INFO/agent1] child process calling self.run()\n",
      "[INFO/agent0] child process calling self.run()\n",
      "[INFO/agent2] child process calling self.run()\n",
      "[INFO/agent0] child process calling self.run()\n",
      "[INFO/agent2] child process calling self.run()\n",
      "[INFO/agent1] Agent agent1 starting\n",
      "[INFO/agent1] Agent agent1 starting\n",
      "[INFO/agent0] Agent agent0 starting\n",
      "[INFO/agent0] Agent agent0 starting\n",
      "[INFO/agent2] Agent agent2 starting\n",
      "[INFO/agent2] Agent agent2 starting\n",
      "[INFO/agent5] child process calling self.run()\n",
      "[INFO/agent5] child process calling self.run()\n",
      "[INFO/agent5] Agent agent5 starting\n",
      "[INFO/agent5] Agent agent5 starting\n",
      "[INFO/agent7] child process calling self.run()\n",
      "[INFO/agent7] child process calling self.run()\n",
      "[INFO/agent7] Agent agent7 starting\n",
      "[INFO/agent7] Agent agent7 starting\n",
      "[INFO/agent9] child process calling self.run()\n",
      "[INFO/agent9] child process calling self.run()\n",
      "[INFO/agent9] Agent agent9 starting\n",
      "[INFO/agent9] Agent agent9 starting\n",
      "[INFO/agent4] child process calling self.run()\n",
      "[INFO/agent4] child process calling self.run()\n",
      "[INFO/agent4] Agent agent4 starting\n",
      "[INFO/agent4] Agent agent4 starting\n",
      "[INFO/agent8] child process calling self.run()\n",
      "[INFO/agent8] child process calling self.run()\n",
      "[INFO/agent10] child process calling self.run()\n",
      "[INFO/agent8] Agent agent8 starting\n",
      "[INFO/agent8] Agent agent8 starting\n",
      "[INFO/agent10] child process calling self.run()\n",
      "[INFO/agent10] Agent agent10 starting\n",
      "[INFO/agent10] Agent agent10 starting\n",
      "[INFO/agent11] child process calling self.run()\n",
      "[INFO/agent11] child process calling self.run()\n",
      "[INFO/agent11] Agent agent11 starting\n",
      "[INFO/agent11] Agent agent11 starting\n",
      "[INFO/agent6] child process calling self.run()\n",
      "[INFO/agent6] child process calling self.run()\n",
      "[INFO/agent6] Agent agent6 starting\n",
      "[INFO/agent6] Agent agent6 starting\n",
      "[INFO/agent3] child process calling self.run()\n",
      "[INFO/agent3] child process calling self.run()\n",
      "[INFO/agent3] Agent agent3 starting\n",
      "[INFO/agent3] Agent agent3 starting\n",
      "[INFO/agent1] agent1 episode:1; loss:-2.736; episode len:000103; total reward:-00085 entropy:00.917 \n",
      "[INFO/agent1] agent1 episode:1; loss:-2.736; episode len:000103; total reward:-00085 entropy:00.917 \n",
      "[INFO/agent7] agent7 episode:1; loss:-11.659; episode len:000106; total reward:-00405 entropy:00.918 \n",
      "[INFO/agent7] agent7 episode:1; loss:-11.659; episode len:000106; total reward:-00405 entropy:00.918 \n",
      "[INFO/agent5] agent5 episode:3; loss:-3.547; episode len:000094; total reward:-00186 entropy:00.922 \n",
      "[INFO/agent5] agent5 episode:3; loss:-3.547; episode len:000094; total reward:-00186 entropy:00.922 \n",
      "[INFO/agent0] agent0 episode:3; loss:-19.496; episode len:000106; total reward:-00320 entropy:00.924 \n",
      "[INFO/agent0] agent0 episode:3; loss:-19.496; episode len:000106; total reward:-00320 entropy:00.924 \n",
      "[INFO/agent6] agent6 episode:2; loss:-3.427; episode len:000126; total reward:-00225 entropy:00.925 \n",
      "[INFO/agent6] agent6 episode:2; loss:-3.427; episode len:000126; total reward:-00225 entropy:00.925 \n",
      "[INFO/agent5] agent5 episode:5; loss:-23.437; episode len:000113; total reward:-00238 entropy:00.921 \n",
      "[INFO/agent5] agent5 episode:5; loss:-23.437; episode len:000113; total reward:-00238 entropy:00.921 \n",
      "[INFO/agent8] agent8 episode:6; loss:-85.838; episode len:000143; total reward:-00161 entropy:00.922 \n",
      "[INFO/agent8] agent8 episode:6; loss:-85.838; episode len:000143; total reward:-00161 entropy:00.922 \n",
      "[INFO/agent9] agent9 episode:7; loss:-31.745; episode len:000155; total reward:-00270 entropy:00.922 \n",
      "[INFO/agent9] agent9 episode:7; loss:-31.745; episode len:000155; total reward:-00270 entropy:00.922 \n",
      "[INFO/agent11] agent11 episode:6; loss:-51.415; episode len:000159; total reward:-00262 entropy:00.918 \n",
      "[INFO/agent11] agent11 episode:6; loss:-51.415; episode len:000159; total reward:-00262 entropy:00.918 \n",
      "[INFO/agent9] agent9 episode:9; loss:-102.724; episode len:000163; total reward:-00278 entropy:00.917 \n",
      "[INFO/agent9] agent9 episode:9; loss:-102.724; episode len:000163; total reward:-00278 entropy:00.917 \n",
      "[INFO/agent9] agent9 episode:10; loss:-30.422; episode len:000166; total reward:-00307 entropy:00.917 \n",
      "[INFO/agent9] agent9 episode:10; loss:-30.422; episode len:000166; total reward:-00307 entropy:00.917 \n",
      "[INFO/agent7] agent7 episode:13; loss:-1.517; episode len:000170; total reward:-00345 entropy:00.916 \n",
      "[INFO/agent7] agent7 episode:13; loss:-1.517; episode len:000170; total reward:-00345 entropy:00.916 \n",
      "[INFO/agent7] agent7 episode:15; loss:26.740; episode len:000172; total reward:-00282 entropy:00.908 \n",
      "[INFO/agent7] agent7 episode:15; loss:26.740; episode len:000172; total reward:-00282 entropy:00.908 \n",
      "[INFO/agent5] agent5 episode:19; loss:10.165; episode len:000200; total reward:-00288 entropy:00.913 \n",
      "[INFO/agent5] agent5 episode:19; loss:10.165; episode len:000200; total reward:-00288 entropy:00.913 \n",
      "[INFO/agent11] agent11 episode:15; loss:04.415; episode len:000169; total reward:-00257 entropy:00.912 \n",
      "[INFO/agent11] agent11 episode:15; loss:04.415; episode len:000169; total reward:-00257 entropy:00.912 \n",
      "[INFO/agent9] agent9 episode:17; loss:06.324; episode len:000194; total reward:-00211 entropy:00.912 \n",
      "[INFO/agent9] agent9 episode:17; loss:06.324; episode len:000194; total reward:-00211 entropy:00.912 \n",
      "[INFO/agent6] agent6 episode:15; loss:-0.912; episode len:000171; total reward:-00193 entropy:00.912 \n",
      "[INFO/agent6] agent6 episode:15; loss:-0.912; episode len:000171; total reward:-00193 entropy:00.912 \n",
      "[INFO/agent10] agent10 episode:17; loss:-4.451; episode len:000191; total reward:-00211 entropy:00.910 \n",
      "[INFO/agent10] agent10 episode:17; loss:-4.451; episode len:000191; total reward:-00211 entropy:00.910 \n",
      "[INFO/agent7] agent7 episode:22; loss:-15.608; episode len:000202; total reward:-00193 entropy:00.912 \n",
      "[INFO/agent7] agent7 episode:22; loss:-15.608; episode len:000202; total reward:-00193 entropy:00.912 \n",
      "[INFO/agent5] agent5 episode:22; loss:-10.745; episode len:000208; total reward:-00233 entropy:00.913 \n",
      "[INFO/agent5] agent5 episode:22; loss:-10.745; episode len:000208; total reward:-00233 entropy:00.913 \n",
      "[INFO/agent9] agent9 episode:24; loss:-89.659; episode len:000201; total reward:-00169 entropy:00.932 \n",
      "[INFO/agent9] agent9 episode:24; loss:-89.659; episode len:000201; total reward:-00169 entropy:00.932 \n",
      "[INFO/agent9] agent9 episode:28; loss:00.195; episode len:000201; total reward:-00187 entropy:00.947 \n",
      "[INFO/agent9] agent9 episode:28; loss:00.195; episode len:000201; total reward:-00187 entropy:00.947 \n",
      "[INFO/agent1] agent1 episode:31; loss:-90.749; episode len:000243; total reward:-00292 entropy:00.937 \n",
      "[INFO/agent1] agent1 episode:31; loss:-90.749; episode len:000243; total reward:-00292 entropy:00.937 \n",
      "[INFO/agent6] agent6 episode:28; loss:55.090; episode len:000232; total reward:-00274 entropy:00.931 \n",
      "[INFO/agent6] agent6 episode:28; loss:55.090; episode len:000232; total reward:-00274 entropy:00.931 \n",
      "[INFO/agent0] agent0 episode:35; loss:01.196; episode len:000223; total reward:-00188 entropy:00.865 \n",
      "[INFO/agent0] agent0 episode:35; loss:01.196; episode len:000223; total reward:-00188 entropy:00.865 \n",
      "[INFO/agent2] agent2 episode:35; loss:-0.375; episode len:000244; total reward:-00085 entropy:00.846 \n",
      "[INFO/agent2] agent2 episode:35; loss:-0.375; episode len:000244; total reward:-00085 entropy:00.846 \n",
      "[INFO/agent0] agent0 episode:37; loss:02.044; episode len:000223; total reward:-00173 entropy:00.849 \n",
      "[INFO/agent0] agent0 episode:37; loss:02.044; episode len:000223; total reward:-00173 entropy:00.849 \n",
      "[INFO/agent9] agent9 episode:37; loss:-14.255; episode len:000201; total reward:-00100 entropy:00.834 \n",
      "[INFO/agent9] agent9 episode:37; loss:-14.255; episode len:000201; total reward:-00100 entropy:00.834 \n",
      "[INFO/agent4] agent4 episode:38; loss:32.578; episode len:000241; total reward:-00252 entropy:00.824 \n",
      "[INFO/agent4] agent4 episode:38; loss:32.578; episode len:000241; total reward:-00252 entropy:00.824 \n",
      "[INFO/agent9] agent9 episode:42; loss:-0.826; episode len:000210; total reward:-00129 entropy:00.808 \n",
      "[INFO/agent9] agent9 episode:42; loss:-0.826; episode len:000210; total reward:-00129 entropy:00.808 \n",
      "[INFO/agent7] agent7 episode:46; loss:00.775; episode len:000237; total reward:-00199 entropy:00.788 \n",
      "[INFO/agent7] agent7 episode:46; loss:00.775; episode len:000237; total reward:-00199 entropy:00.788 \n",
      "[INFO/agent10] agent10 episode:49; loss:-1.437; episode len:000224; total reward:-00065 entropy:00.632 \n",
      "[INFO/agent10] agent10 episode:49; loss:-1.437; episode len:000224; total reward:-00065 entropy:00.632 \n",
      "[INFO/agent9] agent9 episode:50; loss:-0.150; episode len:000224; total reward:-00091 entropy:00.608 \n",
      "[INFO/agent9] agent9 episode:50; loss:-0.150; episode len:000224; total reward:-00091 entropy:00.608 \n",
      "[INFO/agent4] agent4 episode:57; loss:-0.484; episode len:000241; total reward:-00015 entropy:00.381 \n",
      "[INFO/agent4] agent4 episode:57; loss:-0.484; episode len:000241; total reward:-00015 entropy:00.381 \n",
      "[INFO/agent4] agent4 Total rewards > 0, early stopping!\n",
      "[INFO/agent4] agent4 Total rewards > 0, early stopping!\n",
      "[INFO/agent4] process shutting down\n",
      "[INFO/agent4] process shutting down\n",
      "[INFO/agent4] process exiting with exitcode 0\n",
      "[INFO/agent4] process exiting with exitcode 0\n",
      "[INFO/agent7] agent7 episode:64; loss:-0.088; episode len:000237; total reward:-00042 entropy:00.252 \n",
      "[INFO/agent7] agent7 episode:64; loss:-0.088; episode len:000237; total reward:-00042 entropy:00.252 \n",
      "[INFO/agent9] agent9 episode:62; loss:-0.050; episode len:000224; total reward:-00029 entropy:00.237 \n",
      "[INFO/agent9] agent9 episode:62; loss:-0.050; episode len:000224; total reward:-00029 entropy:00.237 \n",
      "[INFO/agent8] agent8 Total rewards > 0, early stopping!\n",
      "[INFO/agent8] agent8 Total rewards > 0, early stopping!\n",
      "[INFO/agent8] process shutting down\n",
      "[INFO/agent8] process shutting down\n",
      "[INFO/agent8] process exiting with exitcode 0\n",
      "[INFO/agent8] process exiting with exitcode 0\n",
      "[INFO/agent0] agent0 episode:66; loss:-7.936; episode len:000220; total reward:-00028 entropy:00.122 \n",
      "[INFO/agent0] agent0 episode:66; loss:-7.936; episode len:000220; total reward:-00028 entropy:00.122 \n",
      "[INFO/agent9] agent9 episode:67; loss:63.754; episode len:000214; total reward:-00082 entropy:00.113 \n",
      "[INFO/agent9] agent9 episode:67; loss:63.754; episode len:000214; total reward:-00082 entropy:00.113 \n",
      "[INFO/agent0] agent0 episode:67; loss:160.555; episode len:000226; total reward:-00067 entropy:00.113 \n",
      "[INFO/agent0] agent0 episode:67; loss:160.555; episode len:000226; total reward:-00067 entropy:00.113 \n",
      "[INFO/agent11] agent11 episode:66; loss:66.551; episode len:000219; total reward:-00121 entropy:00.137 \n",
      "[INFO/agent11] agent11 episode:66; loss:66.551; episode len:000219; total reward:-00121 entropy:00.137 \n",
      "[INFO/agent5] agent5 episode:74; loss:-21.195; episode len:000179; total reward:-00222 entropy:00.148 \n",
      "[INFO/agent5] agent5 episode:74; loss:-21.195; episode len:000179; total reward:-00222 entropy:00.148 \n",
      "[INFO/agent2] agent2 episode:71; loss:00.191; episode len:000197; total reward:-00238 entropy:00.134 \n",
      "[INFO/agent2] agent2 episode:71; loss:00.191; episode len:000197; total reward:-00238 entropy:00.134 \n",
      "[INFO/agent3] agent3 episode:75; loss:29.452; episode len:000160; total reward:-00139 entropy:00.133 \n",
      "[INFO/agent3] agent3 episode:75; loss:29.452; episode len:000160; total reward:-00139 entropy:00.133 \n",
      "[INFO/agent10] agent10 episode:74; loss:-0.052; episode len:000215; total reward:-00092 entropy:00.127 \n",
      "[INFO/agent10] agent10 episode:74; loss:-0.052; episode len:000215; total reward:-00092 entropy:00.127 \n",
      "[INFO/agent11] agent11 episode:74; loss:00.065; episode len:000199; total reward:-00130 entropy:00.126 \n",
      "[INFO/agent11] agent11 episode:74; loss:00.065; episode len:000199; total reward:-00130 entropy:00.126 \n",
      "[INFO/agent6] agent6 episode:76; loss:00.519; episode len:000217; total reward:-00066 entropy:00.111 \n",
      "[INFO/agent6] agent6 episode:76; loss:00.519; episode len:000217; total reward:-00066 entropy:00.111 \n",
      "[INFO/agent3] agent3 episode:81; loss:-0.012; episode len:000158; total reward:-00077 entropy:00.097 \n",
      "[INFO/agent3] agent3 episode:81; loss:-0.012; episode len:000158; total reward:-00077 entropy:00.097 \n",
      "[INFO/agent6] agent6 episode:78; loss:00.222; episode len:000217; total reward:-00031 entropy:00.082 \n",
      "[INFO/agent6] agent6 episode:78; loss:00.222; episode len:000217; total reward:-00031 entropy:00.082 \n",
      "[INFO/agent5] agent5 episode:86; loss:-0.141; episode len:000161; total reward:-00023 entropy:00.074 \n",
      "[INFO/agent5] agent5 episode:86; loss:-0.141; episode len:000161; total reward:-00023 entropy:00.074 \n",
      "[INFO/agent6] agent6 Total rewards > 0, early stopping!\n",
      "[INFO/agent6] agent6 Total rewards > 0, early stopping!\n",
      "[INFO/agent6] process shutting down\n",
      "[INFO/agent6] process shutting down\n",
      "[INFO/agent6] process exiting with exitcode 0\n",
      "[INFO/agent6] process exiting with exitcode 0\n",
      "[INFO/agent10] agent10 Total rewards > 0, early stopping!\n",
      "[INFO/agent10] agent10 Total rewards > 0, early stopping!\n",
      "[INFO/agent10] process shutting down\n",
      "[INFO/agent10] process shutting down\n",
      "[INFO/agent10] process exiting with exitcode 0\n",
      "[INFO/agent10] process exiting with exitcode 0\n",
      "[INFO/agent11] agent11 episode:82; loss:-0.023; episode len:000194; total reward:-00045 entropy:00.057 \n",
      "[INFO/agent11] agent11 episode:82; loss:-0.023; episode len:000194; total reward:-00045 entropy:00.057 \n",
      "[INFO/agent1] agent1 episode:90; loss:-0.071; episode len:000189; total reward:-00022 entropy:00.015 \n",
      "[INFO/agent1] agent1 episode:90; loss:-0.071; episode len:000189; total reward:-00022 entropy:00.015 \n",
      "[INFO/agent0] agent0 episode:93; loss:41.444; episode len:000199; total reward:-00079 entropy:-0.094 \n",
      "[INFO/agent0] agent0 episode:93; loss:41.444; episode len:000199; total reward:-00079 entropy:-0.094 \n",
      "[INFO/agent5] process shutting down\n",
      "[INFO/agent5] process shutting down\n",
      "[INFO/agent5] process exiting with exitcode 0\n",
      "[INFO/agent5] process exiting with exitcode 0\n",
      "[INFO/agent7] process shutting down\n",
      "[INFO/agent7] process shutting down\n",
      "[INFO/agent7] process exiting with exitcode 0\n",
      "[INFO/agent7] process exiting with exitcode 0\n",
      "[INFO/agent1] process shutting down\n",
      "[INFO/agent1] process shutting down\n",
      "[INFO/agent1] process exiting with exitcode 0\n",
      "[INFO/agent1] process exiting with exitcode 0\n",
      "[INFO/agent2] process shutting down\n",
      "[INFO/agent2] process shutting down\n",
      "[INFO/agent2] process exiting with exitcode 0\n",
      "[INFO/agent2] process exiting with exitcode 0\n",
      "[INFO/agent11] process shutting down\n",
      "[INFO/agent11] process shutting down\n",
      "[INFO/agent11] process exiting with exitcode 0\n",
      "[INFO/agent11] process exiting with exitcode 0\n",
      "[INFO/agent3] process shutting down\n",
      "[INFO/agent3] process shutting down\n",
      "[INFO/agent3] process exiting with exitcode 0\n",
      "[INFO/agent3] process exiting with exitcode 0\n",
      "[INFO/agent0] process shutting down\n",
      "[INFO/agent0] process shutting down\n",
      "[INFO/agent0] process exiting with exitcode 0\n",
      "[INFO/agent0] process exiting with exitcode 0\n",
      "[INFO/agent9] process shutting down\n",
      "[INFO/agent9] process shutting down\n",
      "[INFO/agent9] process exiting with exitcode 0\n",
      "[INFO/agent9] process exiting with exitcode 0\n"
     ]
    }
   ],
   "source": [
    "model_ll = a3c('LunarLanderContinuous-v2',\n",
    "            128,\n",
    "            n_agents=os.cpu_count(),\n",
    "            num_episodes=100,\n",
    "            early_stopping_reward=0,\n",
    "            train_every_step=8,\n",
    "            max_episode_length=300,\n",
    "            entropy_loss_weight=0.,\n",
    "            gamma=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "with gym.make('LunarLanderContinuous-v2') as env:\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = model_ll.forward(torch.tensor(state))[1]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}