{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DDPG\n",
    "\n",
    "DDPG implements a variant of Q learning by optimizing both action value and policy networks.\n",
    "It is designed to support continuous action policies."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.3157, grad_fn=<SqueezeBackward1>)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden):\n",
    "        super().__init__()\n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_size + action_size, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.nn(torch.concat([state, action], dim=-1)).squeeze(dim=-1)\n",
    "\n",
    "\n",
    "QModel(2, 2, 4).forward(torch.ones(2), torch.zeros(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 2.       , -1.7447393], dtype=float32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PolicyModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden, action_scaling):\n",
    "        super().__init__()\n",
    "        self.action_scaling = action_scaling\n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_size, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, action_size),\n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.nn(state) * self.action_scaling\n",
    "\n",
    "    def act(self, state, policy_std=1e-6):\n",
    "        action = torch.clip(\n",
    "            torch.distributions.Normal(self.forward(state), policy_std).sample(),\n",
    "            -self.action_scaling,\n",
    "            self.action_scaling,\n",
    "        )\n",
    "        return action.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "PolicyModel(2, 2, 4, 2).act(torch.ones(2), 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0., 2.], device='cuda:0'),\n tensor([0., 0.], device='cuda:0'),\n tensor([0., 2.], device='cuda:0'),\n tensor([0., 2.], device='cuda:0'),\n tensor([0., 2.], device='cuda:0'))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, o, a, size):\n",
    "        self.buf = []\n",
    "        self.size = size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buf.append((state, action, reward, next_state, done))\n",
    "        return self\n",
    "\n",
    "    def crop_buffer(self):\n",
    "        if len(self.buf) > self.size:\n",
    "            self.buf = self.buf[-self.size:]\n",
    "\n",
    "    def gen_batch(self, batch_size):\n",
    "        np.random.shuffle(self.buf)\n",
    "        b = self.buf[:batch_size]\n",
    "        states = torch.tensor([b[y][0] for y in range(len(b))], dtype=torch.float32).to(DEVICE)\n",
    "        actions = torch.tensor([b[y][1] for y in range(len(b))], dtype=torch.float32).to(DEVICE)\n",
    "        rewards = torch.tensor([b[y][2] for y in range(len(b))], dtype=torch.float32).to(DEVICE)\n",
    "        next_states = torch.tensor([b[y][3] for y in range(len(b))], dtype=torch.float32).to(DEVICE)\n",
    "        dones = torch.tensor([b[y][4] for y in range(len(b))], dtype=torch.float32).to(DEVICE)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "\n",
    "ReplayBuffer(0, 0, 3).add(0, 0, 0, 0, 0).add(1, torch.tensor(0), 1, 1, 1).add(2, torch.tensor(0), 2, 2, 2).gen_batch(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import copy\n",
    "from stable_baselines3.common.utils import polyak_update\n",
    "import gym\n",
    "\n",
    "def test_ddpg(env_name, pmodel, render=True):\n",
    "    with gym.make(env_name) as env:\n",
    "        total_rewards = []\n",
    "        for j in range(5):\n",
    "            total_reward = 0\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = pmodel.act(torch.as_tensor(state, dtype=torch.float32, device=DEVICE))\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                if render:\n",
    "                    env.render()\n",
    "            total_rewards.append(total_reward)\n",
    "        return np.mean(total_rewards), env\n",
    "\n",
    "\n",
    "def ddpg(env_name,\n",
    "         max_episodes,\n",
    "         max_steps,\n",
    "         hidden_size=128,\n",
    "         policy_std=0.1,\n",
    "         train_every_step=50,\n",
    "         batch_size=100,\n",
    "         target_update_rate=0.01,\n",
    "         gamma=0.9,\n",
    "         replay_buffer_size=100000,\n",
    "         early_stopping_reward=1e9,\n",
    "         act_after_steps=10000,\n",
    "         train_after_steps=1000):\n",
    "    with gym.make(env_name) as env:\n",
    "        assert min(env.action_space.low) == max(env.action_space.low)\n",
    "        assert min(env.action_space.high) == max(env.action_space.high)\n",
    "        assert np.all(np.abs(env.action_space.low) == np.abs(env.action_space.high))\n",
    "\n",
    "        pmodel = PolicyModel(env.observation_space.shape[0], env.action_space.shape[0], hidden_size, env.action_space.high[0]).to(DEVICE)\n",
    "        qmodel = QModel(env.observation_space.shape[0], env.action_space.shape[0], hidden_size).to(DEVICE)\n",
    "        print(pmodel)\n",
    "        optim_p = torch.optim.Adam(pmodel.parameters())\n",
    "        optim_q = torch.optim.Adam(qmodel.parameters())\n",
    "\n",
    "        target_pmodel = copy.deepcopy(pmodel)\n",
    "        target_qmodel = copy.deepcopy(qmodel)\n",
    "\n",
    "        for p in target_qmodel.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in target_pmodel.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        total_rewards = []\n",
    "\n",
    "        buffer = ReplayBuffer(env.observation_space.shape[0], env.action_space.shape[0], replay_buffer_size)\n",
    "        global_step = 0\n",
    "        progress = tqdm(range(max_episodes), total=max_episodes)\n",
    "        for _ in progress:\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            for step in range(max_steps):\n",
    "\n",
    "                if global_step > act_after_steps:\n",
    "                    action = pmodel.act(torch.as_tensor(state, dtype=torch.float32, device=DEVICE), policy_std)\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                buffer.add(state, action, reward, next_state, done)\n",
    "                global_step += 1\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if global_step % train_every_step == 0 and global_step > train_after_steps:\n",
    "                    for _ in range(train_every_step):\n",
    "                        states, actions, rewards, next_states, dones = buffer.gen_batch(batch_size)\n",
    "\n",
    "                        # Q update\n",
    "                        optim_q.zero_grad()\n",
    "                        q = qmodel.forward(states, actions)\n",
    "                        target = rewards\n",
    "                        with torch.no_grad():\n",
    "                            next_state_value = target_qmodel.forward(next_states.to(DEVICE), target_pmodel.forward(next_states.to(DEVICE)))\n",
    "                            target += (1 - dones) * gamma * next_state_value\n",
    "\n",
    "                        q_loss = ((target - q) ** 2).mean()\n",
    "                        q_loss.backward()\n",
    "                        optim_q.step()\n",
    "\n",
    "                        # P update\n",
    "                        q_req_grad = [p.requires_grad for p in qmodel.parameters()]\n",
    "                        for p in qmodel.parameters():\n",
    "                            p.requires_grad = False\n",
    "\n",
    "                        optim_p.zero_grad()\n",
    "                        actions = pmodel.forward(states.to(DEVICE))\n",
    "                        p_loss = -qmodel.forward(states.to(DEVICE), actions).mean()\n",
    "                        p_loss.backward()\n",
    "                        optim_p.step()\n",
    "\n",
    "                        for rg, p in zip(q_req_grad, qmodel.parameters()):\n",
    "                            p.requires_grad = rg\n",
    "\n",
    "                        # update target policies\n",
    "                        polyak_update(qmodel.parameters(), target_qmodel.parameters(), target_update_rate)\n",
    "                        polyak_update(pmodel.parameters(), target_pmodel.parameters(), target_update_rate)\n",
    "\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            buffer.crop_buffer()\n",
    "\n",
    "            test_reward = test_ddpg(env_name, pmodel, render=False)[0]\n",
    "            if test_reward > early_stopping_reward:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "            progress.set_description(f'Test reward:{test_reward:06.2f}')\n",
    "\n",
    "        return target_pmodel, target_qmodel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pendulum-v1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyModel(\n",
      "  (nn): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/30 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1868e0434794ae483100e356956b5ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27247/3515204342.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  states = torch.tensor([b[y][0] for y in range(len(b))], dtype=torch.float32).to(DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "pmodel, qmodel = ddpg(\n",
    "    'Pendulum-v1',\n",
    "     max_episodes=30,\n",
    "     max_steps=500,\n",
    "     hidden_size=256,\n",
    "     policy_std=0.1,\n",
    "     train_every_step=50,\n",
    "     batch_size=100,\n",
    "     target_update_rate=0.005,\n",
    "     gamma=0.99,\n",
    "     replay_buffer_size=100000,\n",
    "     early_stopping_reward=-200\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "env = test_ddpg('Pendulum-v1', pmodel, True)[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7fb945e87970>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render(mode='rgb_array'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HalfCheetah-v2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build_ext\n",
      "PolicyModel(\n",
      "  (nn): Sequential(\n",
      "    (0): Linear(in_features=17, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1024, out_features=6, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8ffd7420b2d474da60fbd5fa17efcb2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29729/3515204342.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  states = torch.tensor([b[y][0] for y in range(len(b))], dtype=torch.float32).to(DEVICE)\n"
     ]
    }
   ],
   "source": [
    "pmodel, qmodel = ddpg(\n",
    "    'HalfCheetah-v2',\n",
    "     max_episodes=50,\n",
    "     max_steps=4000,\n",
    "     hidden_size=1024,\n",
    "     policy_std=0.1,\n",
    "     train_every_step=50,\n",
    "     batch_size=100,\n",
    "     target_update_rate=0.005,\n",
    "     gamma=0.99,\n",
    "     replay_buffer_size=1e6,\n",
    "     early_stopping_reward=1e9\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = test_ddpg('HalfCheetah-v2', pmodel, True)[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(env.render(mode='rgb_array'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}